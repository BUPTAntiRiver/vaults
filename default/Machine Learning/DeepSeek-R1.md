This paper mainly focuses on lots of engineering ingenuity rather than proposing a very new algorithm. Two reasoning models are introduced in this paper: DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero is a model trained via **large scale reinforcement learning (RL) without supervised fine-tuning (SFT)** as a preliminary step. And DeepSeek-R1 incorporates **multi-stage training** and **cold-start data** before RL.
# Introduction
Recently, post-training has appeared as an important component of the full training pipeline.
## Zero
First let's talk about DeepSeek-R1-Zero, they use DeepSeek-V3-Base as the **base model** and **employ [[Post Training with Reinforcement Learning Summary#GRPO|GRPO]]** as the RL framework to improve model performance in reasoning. But it encounters **problems** like **poor readability, and language mixing**. This is very likely due to they only used RL here. In RL we only concern whether such approach can achieve higher reward, but don't care what method it is, so the model might think in anyway it can to reach the correct answer, which makes it hard to be understood by human.
## R1
To address these issues, they introduce DeepSeek-R1. They first collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, they perform the RL stuff like Zero. Upon convergence in RL process, they create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 and then retrain the DeepSeek-V3-Base model. Then they need RL one more time to take prompts into account. A lot of post-training methods are utilized here, try to match them with the common methods mentioned in [[Post-training]].
## Other
They also explored distillation from DeepSeek-R1 to smaller dense models.
# Approach
## Zero: RL on the Base Model
The main exploration is developing LLM reasoning capabilities *without any supervised data*, focusing on their self-evolution through a pure reinforcement learning process. They use GRPO here.
### Reinforcement Learning Algorithm
**Group Relative Policy Optimization** See this in the paper for simplicity.
### Reward Modeling
Two types of rewards: **Accuracy** and **Format**.
### Training Template
They require the model to output reasoning process through a single form prompting.
## R1: RL with Cold Start
Inspired by the promising result of Zero, two natural **questions** arise:
1. Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high quality data as a cold start?
2. How can we train a user friendly model?
### Cold Start
How to gather such high quality data? By Zero! They gather readable output of Zero and post-processing them with human power (which is unclear in the paper).
### Reasoning-oriented Reinforcement Learning
A language consistency reward is added during RL training.
### Make the model usable in all scenario
We are going to SFT and RL again, but this time not focusing on reasoning ability, instead this stage incorporates data from other domains to enhance the model's capabilities in writing, role-playing and other general purpose tasks. The RL also stands for all scenarios. Check the details in the paper.
### Distillation
They tried distillation on R1 to fine-tune smaller models by feeding the data generated by R1 to smaller models.